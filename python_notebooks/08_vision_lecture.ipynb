{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents <a id='top'></a>\n",
    "\n",
    "1. <a href=#intro>Introduction</a>\n",
    "1. <a href=#img-proc>Image Processing</a>\n",
    "1. <a href=#obj-det>Object Detection</a>\n",
    "1. <a href=#ref>References and Links</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "# 1. Introduction\n",
    "<a href=#top>(back to top)</a>\n",
    "\n",
    "Data science has a number of applications in computer vision. These have increased in number and accuray in the past 5 years or so due to the advancements in deep learning (both theory and computational feasibility). In this topic, we shall experiment with some of the applications, utilising existing deep learning models.\n",
    "\n",
    "Here is a short list of applications of computer vision techniques:\n",
    "\n",
    "1. Optical Character Recognition: Reading handwritten documents, car license plate numbers from images.\n",
    "2. Surveillance and traffic monitoring: Monitoring cars on a highway, tracking humans in security cameras for suspicious activity.\n",
    "3. Machine inspection: Automatically detecting damage on components manufactured, such as silicon wafers, etc.\n",
    "\n",
    "[Here](https://www.cs.ubc.ca/~lowe/vision.html) is an old page with a more comprehensive list of applications.\n",
    "\n",
    "In computer vision, the goal is to get a computer to perceive the world as *we* see it. This is not easy even for us to do&mdash;we can get fooled by optical illusions such as the ones below.\n",
    "\n",
    "<img src=\"../figs/Mueller-Lyer-illusion.png\" style=\"width: 250px;\"/><img src=\"../figs/shadow-illusion3.png\" style=\"width: 500px;\"/>\n",
    "\n",
    "In general though, for us, it is possible to do things like pick out faces that we recognise from a photograph of a crowd. But how can we get a computer to do it? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For interactive plotting in JupyterLab, you will need to install ipympl (e.g., pip install ipympl). You will probably have to restart JupyterLab to get widget to work. (Restarting the kernel is not sufficient.)\n",
    "\n",
    "For interactive plotting in Jupyter Notebook, replace widget by notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='img-proc'></a>\n",
    "# 2. Image Processing\n",
    "<a href=#top>(back to top)</a>\n",
    "\n",
    "For the next few sections, let us focus on a set of images of sea-grass, taken from St. John's island by students, as part of a fieldwork trip. Several photographs of randomly sampled sections of the beach were taken. Here are some of the photos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Path('../data/seagrass/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 4)\n",
    "for i, f in enumerate(p.iterdir()):\n",
    "    #print(f)\n",
    "    I = cv.imread(str(f))\n",
    "    #In the case of color images, the decoded images will have the channels stored in **B G R** order.\n",
    "    I_RGB = cv.cvtColor(I, cv.COLOR_BGR2RGB)\n",
    "    if I_RGB.shape[1] > I_RGB.shape[0]:\n",
    "        I_RGB = I_RGB.swapaxes(0, 1)\n",
    "    ax[i].imshow(I_RGB);\n",
    "    ax[i].set_title(str(f.stem))\n",
    "\n",
    "fig.set_size_inches((15, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.imshow(I_RGB);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "One of the goals was to \"count\" the amount of seagrass in each photo. Let's simplify things, and assume that all the GREEN portions are seagrass. How can we do so with a computer? Perhaps we could break the problem down into a few steps:\n",
    "\n",
    "1. Detect the bounding box and crop out the remaining portion of the image.\n",
    "2. Keep only the pixels that fall in the GREEN portion of the colour spectrum.\n",
    "3. Count those pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram Equalisation\n",
    "\n",
    "As we can see, some of the images are bright and some are darker. Equalising the histogram will make it easier to detect edges and boundaries later on. What do we mean by equalising the histogram? First, we convert the RGB colorspace into the HSV (Hue, Saturation and Value) colorspace ([details](https://docs.opencv.org/4.5.3/de/d25/imgproc_color_conversions.html#color_convert_rgb_hsv)). In this space, the \"color\" is represented by hue, just a single coordinate. The Value represents the intensity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = cv.imread(str(p / 'R06.JPG'))\n",
    "I_RGB = cv.cvtColor(I, cv.COLOR_BGR2RGB)\n",
    "I_HSV = cv.cvtColor(I, cv.COLOR_BGR2HSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I_HSV_equal = np.copy(I_HSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I_HSV_equal[:, :, 2] = cv.equalizeHist(I_HSV[:, :, 2])\n",
    "I_RGB_equal = cv.cvtColor(I_HSV_equal, cv.COLOR_HSV2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(221); #plt.subplot(2,2,1)\n",
    "plt.imshow(I_RGB);\n",
    "plt.subplot(222); #plt.subplot(2,2,2)\n",
    "plt.imshow(I_RGB_equal);\n",
    "plt.subplot(223)\n",
    "plt.hist(I_HSV[:, :, 2].ravel());\n",
    "plt.subplot(224)\n",
    "plt.hist(I_HSV_equal[..., 2].ravel());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thresholding\n",
    "\n",
    "Thresholding is a routine operation in image processing - it means that we only keep the pixels that satisy a certain criteria. The rest are zero-ed out. For instance, we may wish to keep only those pixels that fall within the bounding box of an object that we have detected, or only those that are a certain shade of GREEN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define range of green color in HSV\n",
    "lower_green = np.array([ 30,   0,  0])\n",
    "upper_green = np.array([ 90, 255, 90])\n",
    "\n",
    "# Threshold the HSV image to get only green colors\n",
    "mask_hsv = cv.inRange(I_HSV_equal, lower_green, upper_green)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(121);\n",
    "plt.imshow(mask_hsv, cmap='gray');\n",
    "plt.subplot(122);\n",
    "plt.imshow(I_RGB);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define range of green color in HSV\n",
    "lower_green = np.array([ 30,   0,  0])\n",
    "upper_green = np.array([ 90, 255, 90])\n",
    "\n",
    "# Threshold the HSV image to get only green colors\n",
    "mask_hsv = cv.inRange(I_HSV, lower_green, upper_green)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(121);\n",
    "plt.imshow(mask_hsv, cmap='gray');\n",
    "plt.subplot(122);\n",
    "plt.imshow(I_RGB);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edge Detection\n",
    "\n",
    "In this application, we wish to detect the boundary of the rectangular mesh automatically. There are numerous algorithms for detecting edges, but most of them have to do with detecting sharp changes in the gradient of the intensity. One of the commonly used ones is [Harris Edge and Corner detection](https://docs.opencv.org/master/dc/d0d/tutorial_py_features_harris.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I_GRAY = cv.cvtColor(I_RGB_equal, cv.COLOR_RGB2GRAY)\n",
    "I_gray_32 = np.float32(I_GRAY)\n",
    "# Input single-channel 8-bit or floating-point image\n",
    "dst = cv.cornerHarris(I_gray_32, 10, 3, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When R < 0, which happens when λ1 >> λ2 or vice versa, the region is edge\n",
    "retval, out_img = cv.threshold(dst, 0, 255, cv.THRESH_BINARY_INV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kernel = np.ones((3, 3), np.uint8)\n",
    "edges2 = cv.morphologyEx(out_img, cv.MORPH_OPEN, kernel, iterations=5)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(131);\n",
    "plt.imshow(I_GRAY, cmap='gray');\n",
    "plt.subplot(132);\n",
    "plt.imshow(out_img, cmap='gray');\n",
    "plt.subplot(133);\n",
    "plt.imshow(edges2, cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(out_img, cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the edge detection, we performed an \"opening\" to get rid of small specks and attempted to magnify the line thickness.\n",
    "\n",
    "<img src=\"../figs/morph_open.png\" width=200>\n",
    "\n",
    "The opposite of \"opening\" is closing a figure:\n",
    "\n",
    "<img src=\"../figs/morph_close.png\" width=200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation \n",
    "\n",
    "As we proceed, most of the models we use are going to be trained on huge datasets. Remember that the more examples a model sees during its training phase, the better it can learn and generalise. However, it is just not possible to collect all possible samples of images. Imagine if we are training a model to detect cats - could we get all images of all breeds of cats in all poses at all possible angles at all possible resolutions?\n",
    "\n",
    "To get around this, we can augment our training set with variations of our original training images. Many deep learning packages/frameworks already contain such functions (see keras, tensorflow, for instance). We shall use a dedicated package for it, so that we do not have to learn about dataset loaders etc. from those packages.\n",
    "\n",
    "Suppose we wish to generate variations of the following image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hosi = cv.imread('../data/hosico_cat.jpg')\n",
    "hosi = cv.cvtColor(hosi, cv.COLOR_BGR2RGB)\n",
    "plt.imshow(hosi);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imgaug import augmenters as iaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = iaa.Sequential([\n",
    "    iaa.Fliplr(0.5), # horizontal flips\n",
    "    iaa.Crop(percent=(0, 0.1)), # random crops\n",
    "    # Small gaussian blur with random sigma between 0 and 0.5.\n",
    "    # But we only blur about 50% of all images.\n",
    "    iaa.Sometimes(\n",
    "        0.5,\n",
    "        iaa.GaussianBlur(sigma=(0, 0.5))\n",
    "    ),\n",
    "    # Strengthen or weaken the contrast in each image.\n",
    "    iaa.LinearContrast((0.75, 1.5)),\n",
    "    # Add gaussian noise.\n",
    "    # For 50% of all images, we sample the noise once per pixel.\n",
    "    # For the other 50% of all images, we sample the noise per pixel AND\n",
    "    # channel. This can change the color (not only brightness) of the\n",
    "    # pixels.\n",
    "    iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5),\n",
    "    # Make some images brighter and some darker.\n",
    "    # In 20% of all cases, we sample the multiplier once per channel,\n",
    "    # which can end up changing the color of the images.\n",
    "    iaa.Multiply((0.8, 1.2), per_channel=0.2),\n",
    "    # Apply affine transformations to each image.\n",
    "    # Scale/zoom them, translate/move them, rotate them and shear them.\n",
    "    iaa.Affine(\n",
    "        scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\n",
    "        translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n",
    "        rotate=(-25, 25),\n",
    "        shear=(-8, 8)\n",
    "    )\n",
    "], random_order=True)\n",
    "#images_aug = seq(image=im_me)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hosi_list = [hosi, hosi, hosi, hosi, hosi]\n",
    "images_aug = seq(images=hosi_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "for ii in np.arange(5):\n",
    "    plt.subplot(1, 5, ii+1)\n",
    "    plt.imshow(images_aug[ii])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='obj-det'></a>\n",
    "# 3. Object Detection\n",
    "<a href=#top>(back to top)</a>\n",
    "\n",
    "Object detection is done by training deep learning models to detect pre-specified categories of objects. The models use different strategies, and the particular architecture they use for the deep learning  model reflects this.\n",
    "\n",
    "For instance, single-shot detectors use a strategy where they generate multiple \"anchor\" boxes and predict the object categories within them. After that, boxes are combined if they refer to the same object, or dropped if they correspond to \"background\".\n",
    "\n",
    "<img src=\"../figs/ssd.svg\" width=450>\n",
    "\n",
    "Region-based models search for 'interesting regions' instead of brute forcing multiple anchor boxes.\n",
    "\n",
    "<img src=\"../figs/r-cnn.svg\" width=450>\n",
    "\n",
    "A very popular object detection model is called YOLO (You only look once). Let us try it out here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenCV Model Zoo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The opencv library comes with a `dnn` module that contains several pre-trained deep learning models. Here is how we can inspect them and download them. The files and information come from two different git repositories:\n",
    "\n",
    "1. The [opencv_extra](https://github.com/opencv/opencv_extra) repository. (Download the zip file and unzip it on your laptop). This folder contains\n",
    "    * `testdata/dnn/download_models.py`, which can be used to download the configuration file and weights file \n",
    "    for a particular pre-trained model. This file is included as is in the `myscripts` module. \n",
    "1. The main [opencv](https://github.com/opencv/opencv) repository. (Download the zip file and unzip it on your laptop).\n",
    "    * Under the `samples/dnn` folder are several example Python scripts that you can use to run these models.\n",
    "    * One of these files is `samples/dnn/models.yml`. It contains some specifications of the model parameters. These parameters need to be included when calling one of the sample scripts, for instance\n",
    "    `samples/dnn/object_detection.py`. This `models.yml` file is the model zoo. It also contains specifications such as image size, and whether the image needs to be in RGB specification.\n",
    "    * Another set of files are contained in `samples/data/dnn`. These txt files contain the possible prediction classes for some of the object detection models.\n",
    "    \n",
    "It is best if you clone those two repositories onto your computer, but let's see if we can get by without doing that. Here are the relevant files that have been copied for you into the `python_notebooks/` folder:\n",
    "\n",
    "1. `models.yml`\n",
    "2. `classification_classes_ILSVRC2012.txt`\n",
    "3. `object_detection_classes_yolov4.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that we wished to download the YOLOv4 weights. We are going to download the configuration file and the weights using the download_models module that we have copied into scripts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import myscripts.download_models as dm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are all possible models we can download and (theoretically) apply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_names = [x.name for x in dm.models]\n",
    "print('\\n'.join(np.unique(model_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For object detection\n",
    "#yolo3mod = [x for x in dm.models if x.name.startswith('YOLOv3')][0]\n",
    "yolo4mod = [x for x in dm.models if x.name.startswith('YOLOv4')][0]\n",
    "\n",
    "# For image classification\n",
    "googlenetmod = [x for x in dm.models if x.name.startswith('GoogleNet')][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#googlenetmod.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yolo3mod.get()\n",
    "#yolo4mod.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take a closer look at the specification of YOLO version 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!findstr /n object_detection models.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!grep \"YOLO\" -A 10 models.yml\n",
    "!findstr /n YOLO models.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*open('models.yml').readlines()[20:36], sep=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "It expects the input images to have dimensions (416, 416), that the pixels be in  RGB format, but scaled by 0.00392, and no mean centering. The categories of objects that it can pick up are in the file `object_detection_classes_yolov4.txt`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!grep \"googlenet\" -I -A 10 models.yml\n",
    "!findstr /n googlenet models.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*open('models.yml').readlines()[134:148], sep=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast, the GoogLeNet model requires images to be of size (224, 224), to be in BGR format, and to be mean centred. The categories that it predicts from are listed in `classification_classes_ILSVRC2012.txt`.\n",
    "\n",
    "Now let us inspect **how** to run object detection using the sample script from opencv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run opencv-4.x/samples/dnn/object_detection.py --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you run the next command, take note of the following. The opencv GUI window management is a little fragile. When you are done with it, **do not** click on the \"X\" at the top to close the window. Instead, press \"q\" on your keyboard. That would stop the process in the background. Then, return to the jupyter notebook and run this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If you click on the \"X\", you will end up having to restart your kernel.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run opencv-4.x/samples/dnn/object_detection.py --input ../data/cars.jpg --model yolov4.weights \\\n",
    "--nms 0.1 --config yolov4.cfg --scale 0.00392 --width 416 --height 416 --rgb \\\n",
    "--classes object_detection_classes_yolov4.txt yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run opencv-4.x/samples/dnn/object_detection.py --input ../data/2018-07-18-1350.jpg --model yolov4.weights \\\n",
    "--config yolov4.cfg --scale 0.00392 --width 416 --height 416 --rgb \\\n",
    "--classes object_detection_classes_yolov4.txt yolo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try any image that you have on your laptop or phone. When you are done, you can even use the camera on your laptop to detect things in realtime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run opencv-4.x/samples/dnn/object_detection.py --model yolov4.weights \\\n",
    "--config yolov4.cfg --scale 0.00392 --width 416 --height 416 --rgb \\\n",
    "--classes object_detection_classes_yolov4.txt yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run opencv-4.x/samples/dnn/classification.py --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run opencv-4.x/samples/dnn/classification.py --input ../data/cars.jpg \\\n",
    "--model bvlc_googlenet.caffemodel --config bvlc_googlenet.prototxt \\\n",
    "--scale 1.00 --width 224 --height 224 --mean 104 117 123 \\\n",
    "--classes classification_classes_ILSVRC2012.txt googlenet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Detections Programmatically\n",
    "\n",
    "Instead of running things from the command line, sometimes we need to run the detection through a program. We need to figure out how the `object_detection.py` works. This is a common experience in data science - having to read and understand someone else's code, and to then hack it to do what we need. \n",
    "\n",
    "Here's how we can step through the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run opencv-4.x/samples/dnn/object_detection.py --input ../data/cars.jpg --model yolov4.weights \\\n",
    "--config yolov4.cfg --scale 0.00392 --width 416 --height 416 --rgb \\\n",
    "--classes object_detection_classes_yolov4.txt yolo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have done that, we can identify the main steps in the program as :\n",
    "1. instantiate the dnn object\n",
    "2. convert the image to a blob\n",
    "3. run the postprocess() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import myscripts.vision as vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# object_detection.py line 89\n",
    "nn = cv.dnn.readNet('yolov4.weights', 'yolov4.cfg')\n",
    "# object_detection.py line 85\n",
    "with open('opencv-master/samples/data/dnn/object_detection_classes_yolov4.txt', 'rt') as f:\n",
    "    classes = f.read().rstrip('\\n').split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f1 = cv.imread('../data/football1.png')\n",
    "#f1 = cv.imread('../data/busy_street_Manhattan.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(cv.cvtColor(f1, cv.COLOR_BGR2RGB));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# object_detection.py lines 272, 276, 284(92)\n",
    "blob = cv.dnn.blobFromImage(f1, size=(416, 416), swapRB=True, ddepth=cv.CV_8U)\n",
    "nn.setInput(blob, scalefactor=0.00392 )\n",
    "outs = nn.forward(nn.getUnconnectedOutLayersNames())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(vs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs.postprocess(f1, outs, classes, nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(cv.cvtColor(f1, cv.COLOR_BGR2RGB));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing perspective of an image\n",
    "\n",
    "In computer vision, we have to remember that we are projecting a 3-dimensional world onto a 2-D plane. At times, we need to change the perspective of a plane in order to compute coordinates correctly. If we can identify a mapping of 4 points from the plane we want onto the view we desire, then we can do so. Consider obtaining a top-down view of the field from the above image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv.imread('../data/football1.png')\n",
    "pts = np.array([[45, 121], [48, 238], [206, 230], [155, 119]], np.int32)\n",
    "pts = pts.reshape((-1, 1, 2))\n",
    "img2 = cv.polylines(img, [pts], True, (255, 0, 0), 1 )\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(cv.cvtColor(img2, cv.COLOR_BGR2RGB));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that the blue polygon above should map to a rectangle in the top-down view, with the **ellipse as a circle**. So with a little trial and error, we can obtain this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pts1 = np.float32([[45, 121], [48, 238], [206, 230], [155, 118]])\n",
    "#pts2 = np.float32([[45, 100], [45, 220], [200, 220], [200, 100]])\n",
    "pts2 = np.float32([[45, 100], [45, 220], [77, 220], [77, 100]])\n",
    "\n",
    "M = cv.getPerspectiveTransform(pts1, pts2)\n",
    "dst = cv.warpPerspective(img, M, (504, 360))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(cv.cvtColor(dst[:, :300], cv.COLOR_BGR2RGB));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cv.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ref'></a>\n",
    "# 4. References\n",
    "<a href=#top>(back to top)</a>\n",
    "\n",
    "1. A classic [reference](http://szeliski.org/Book/) on computer vision.\n",
    "2. The opencv [website](https://docs.opencv.org/master/index.html) contains tutorials on the package.\n",
    "3. An alternative to opencv is [gluon-cv](https://gluon-cv.mxnet.io/).\n",
    "4. An [online textbook](https://d2l.ai/index.html) on deep learning, with examples in mxnet and pytorch, written by Amazon researchers. It is used in a course at Berkeley. It has a big chapter on deep learning in computer vision."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
